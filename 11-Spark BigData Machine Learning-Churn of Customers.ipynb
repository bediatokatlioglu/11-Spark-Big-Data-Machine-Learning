{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31353f9b",
   "metadata": {},
   "source": [
    "<img src=\"PySpark.avif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d91f7",
   "metadata": {},
   "source": [
    "### Spark ve PySpark Nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d1c27",
   "metadata": {},
   "source": [
    "Apache Spark, büyük veri işleme ve analizi için kullanılan açık kaynaklı bir veri işleme çerçevesidir. Spark, büyük veri kümelerini hızlı, paralel ve dağıtık bir şekilde işlemek, analiz etmek ve modellemek için tasarlanmıştır. \n",
    "\n",
    "PySpark ise, büyük veri işleme ve analizi için kullanılan Apache Spark'ın Python API'sidir. Spark, dağıtık veri işleme, yüksek performanslı hesaplama ve büyük veri analizi için tasarlanmış bir açık kaynaklı bir veri işleme çerçevesidir. PySpark ise Python programlama diliyle Spark platformuna erişim sağlayan bir kütüphanedir. İşte PySpark'ın temel özellikleri:\n",
    "\n",
    "1. **Dağıtık İşleme:** Spark, büyük veri kümelerini parçalara ayırarak ve bu parçaları birden fazla bilgisayar üzerinde işleyerek paralel işlem yapabilir. Bu sayede işlemler daha hızlı ve ölçeklenebilir hale gelir.\n",
    "\n",
    "2. **Yüksek Hız:** Spark, veriyi bellekte (RAM) tutarak işlem yapar. Bu, geleneksel disk tabanlı veri işleme sistemlerine göre çok daha hızlı sonuçlar elde etmenizi sağlar.\n",
    "\n",
    "3. **Çeşitli Veri Kaynakları:** PySpark, çeşitli veri kaynaklarına (CSV, JSON, Parquet, Hive, HBase vb.) erişim sağlar ve bu verileri Spark veri yapıları olan RDD (Resilient Distributed Dataset) veya DataFrame formatında işleyebilirsiniz.\n",
    "\n",
    "4. **Yüksek Seviyeli API:** PySpark, Python programlama dilini kullandığı için kullanımı kolaydır ve geniş bir Python topluluğu tarafından desteklenir. Bu, geliştirme süreçlerini hızlandırır.\n",
    "\n",
    "5. **Makine Öğrenimi ve Büyük Veri Analizi:** Spark, geniş bir veri işleme kütüphanesine sahiptir. PySpark üzerinden Spark MLlib'i kullanarak makine öğrenimi modelleri oluşturabilir ve büyük veri kümesi üzerinde analizler yapabilirsiniz.\n",
    "\n",
    "6. **Dağıtık Veri Setleri:** Spark, RDD ve DataFrame gibi yapılarla veriyi temsil eder. Bu yapılar, işlemlerinizi paralel olarak işlemek için kullanılır.\n",
    "\n",
    "7. **Veri Akışı:** Spark Streaming, gerçek zamanlı veri akışını işlemek için kullanılan bir modüldür. Bu sayede veri akışlarını anlık olarak işleyebilirsiniz.\n",
    "\n",
    "8. **Graf İşleme:** Spark GraphX, büyük çaplı graf verilerini işlemek ve analiz etmek için kullanılır.\n",
    "\n",
    "PySpark ile işe başlamak için, öncelikle bir Spark oturumu oluşturmanız gerekmektedir. Bu oturum, Spark bağlantısını kurmanıza ve Spark üzerinde işlem yapmanıza olanak sağlar. Daha sonra veri yükleme, veri işleme, analiz ve model oluşturma gibi adımları gerçekleştirebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53a394",
   "metadata": {},
   "source": [
    "# Projemiz: Spark ile Büyük Veri olan \"Churn\" Tahmini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52799ae",
   "metadata": {},
   "source": [
    "Spark oturumu açtıktan sonra izlenecek adımları aşağıdaki gibi özetleyebiliriz:\n",
    "Elbette, işte zamirleri kullanarak PySpark kullanarak Büyük Veri Müşteri Kaybı (Churn) tahmini projesinin adımları:\n",
    "\n",
    "1. **Veri Hazırlığı:**\n",
    "   - İlgili verileri toplayarak veri kaynaklarından verileri alacağız.\n",
    "   - Bu verileri PySpark DataFrame'e yükleyeceğiz veya oluşturacağız.\n",
    "   - Verileri incelemek ve işlem yapmak için keşifsel veri analizi (EDA) gerçekleştireceğiz. Eksik değerler, aykırı değerler ve kategorik değişkenler hakkında işlemler yapacağız.\n",
    "\n",
    "2. **Veri Ön İşleme:**\n",
    "   - Kategorik değişkenleri sayısal değerlere dönüştürmemiz gerekecek. Bunun için etiket kodlamayı veya tekil kodlamayı kullanabiliriz.\n",
    "   - Modelin performansını artırabilecek yeni özellikler oluşturmak için özellik seçimi ve mühendisliği yapacağız.\n",
    "\n",
    "3. **Veri Bölünmesi:**\n",
    "   - Veriyi eğitim ve test kümelerine ayıracağız. Genellikle verinin %70-80'ini eğitim, %20-30'unu test olarak ayırabiliriz.\n",
    "\n",
    "4. **Model Oluşturma:**\n",
    "   - GBTClassifier gibi bir makine öğrenimi modelini seçeceğiz ve ilgili parametreleri ayarlayacağız.\n",
    "   - Eğitim verilerini kullanarak modeli eğiteceğiz.\n",
    "\n",
    "5. **Model Değerlendirmesi:**\n",
    "   - Test verilerini kullanarak modelin performansını değerlendireceğiz. Doğruluk, hassasiyet, kesinlik, geri çağırma gibi metrikleri kullanacağız.\n",
    "   - Sonuçları anlamak için confusion matrix gibi görselleştirmeleri kullanacağız.\n",
    "\n",
    "6. **Model Ayarlaması ve Optimizasyon:**\n",
    "   - Modelin performansını artırmak için hiperparametre ayarlamaları yapacağız. GridSearch veya RandomizedSearch gibi yöntemleri kullanabiliriz.\n",
    "   - Aşırı öğrenmeyi önlemek için düzenleme (regularization) tekniklerini (örneğin, ağaç sayısını sınırlamak) uygulayabiliriz.\n",
    "\n",
    "7. **Sonuçların Sunumu:**\n",
    "   - Modelin sonuçlarını yönetim veya ilgili paydaşlarla paylaşmak için etkili bir sunum hazırlayacağız.\n",
    "   - Önemli özellikleri, işaretçileri ve modelin tahmin yeteneğini vurgulayacağız.\n",
    "\n",
    "8. **Üretim Ortamına Entegrasyon:**\n",
    "   - Başarılı bir model elde edersek, modeli üretim ortamına entegre etmek için gerekli adımları takip edeceğiz.\n",
    "   - Modeli kullanıma sunmak için gereken işlemleri gerçekleştireceğiz.\n",
    "\n",
    "9. **Sürekli İyileştirme:**\n",
    "   - Modeli düzenli olarak gözden geçirip yeni verilerle güncelleyeceğiz. Müşteri davranışı zamanla değişebilir, bu nedenle modelin güncelliğini sağlayacağız.\n",
    "\n",
    "Bu adımları takip ederek, PySpark kullanarak Büyük Veri Müşteri Kaybı tahmini projesini gerçekleştireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb81f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İndirmeler\n",
    "!pip install pycaret\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c52d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kütüphaneler\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyspark  \n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, GBTClassifier, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8115f6f",
   "metadata": {},
   "source": [
    "## 1. **Veri Hazırlığı:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark oturumu açıyoruz:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY = \"10g\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Churn\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebaa43",
   "metadata": {},
   "source": [
    "Bu kod örneği PySpark ile bir Spark oturumu (Spark Session) oluşturmayı gösteriyor. Bir Spark oturumu, Spark işlemlerini ve uygulamalarını yürütmek için kullanılan temel arayüzdür. Kodun her bir bölümünü ayrıntılı bir şekilde açıklayalım:\n",
    "\n",
    "1. **MAX_MEMORY Tanımı:**\n",
    "   Bu kısım, bellek sınırlarını belirlemek için kullanılan bir sabiti tanımlar. `\"10g\"` ifadesi, 10 gigabayt bellek miktarını temsil eder. Bu, Spark oturumunda kullanılacak toplam bellek miktarını belirtir.\n",
    "\n",
    "2. **SparkSession Oluşturma:**\n",
    "   - `SparkSession`: Bu sınıf, Spark uygulamaları için temel giriş noktasıdır. Uygulamalarınızı yürütmek ve Spark'a erişmek için kullanılır.\n",
    "   - `.builder`: SparkSession oluşturmak için kullanılan bir yapılandırıcı nesnesi oluşturur.\n",
    "   - `.appName(\"Churn\")`: Uygulama adını belirtir. Bu isim Spark UI'da görünecektir.\n",
    "   - `.config(\"spark.executor.memory\", MAX_MEMORY)`: Bu yöntem, Spark yapılandırmasını ayarlar. Burada \"spark.executor.memory\" yapılandırmasını \"10g\" olarak ayarlıyoruz. Bu, her bir Spark işçi sürecine tahsis edilen bellek miktarını belirtir.\n",
    "   - `.config(\"spark.driver.memory\", MAX_MEMORY)`: Benzer şekilde, \"spark.driver.memory\" yapılandırmasını da \"10g\" olarak ayarlıyoruz. Bu, sürücü programın (ana işlem) kullanabileceği bellek miktarını belirtir.\n",
    "   - `.getOrCreate()`: Eğer mevcut bir Spark oturumu varsa, onu alır; yoksa yeni bir Spark oturumu oluşturur.\n",
    "\n",
    "Bu kodun amacı, yüksek düzeyde yapılandırılmış bir Spark oturumu oluşturmak ve bu oturumda tanımlanan bellek sınırlarını belirlemektir. Bellek sınırları, Spark işlemlerinin ve hesaplamalarının verimli ve dengeli bir şekilde yapılmasını sağlamak için önemlidir. Özellikle büyük veri işlemleri yürütüldüğünde, bellek ayarlarını doğru bir şekilde yapılandırmak büyük önem taşır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bcf5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi okuyoruz\n",
    "df= spark.read.format(\"csv\").option(\"header\",\"true\").load(\"churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff484959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c677fa8",
   "metadata": {},
   "source": [
    "### EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satır sayısı\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sutun Sayısı\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b939d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info gibi bilgiler için\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5228c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bu kod, _c0 adındaki bir sütunu index olarak yeniden adlandırmak için kullanılır. \n",
    "# Bu tür ad değişiklikleri, verinin daha iyi anlaşılabilir ve yönetilebilir olmasını sağlamak için yaygın olarak kullanılır. \n",
    "df=df.withColumnRenamed(\"_c0\",\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Burada pthondaki unique gibi sutun içindeki benzersizliklere bakacağız. \n",
    "#Çünkü verinin tekrar edip etmediğini bilerek daha sıhhatli sonuçlara ulaşırız, \n",
    "#Names ı neden seçtik çünkü diğer sutunlarda benzerlik olabilir ama isim benzerliği nadir rastlanan bir durumdur.\n",
    "df.select(\"names\").distinct().count() #names sütununda benzesiz olan değerlerin sayısını hesaplar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc07ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#900 olan veri 899 çıktığı için isimde bir tekrar etme sözkonusu onu bulmak için isimleri gruplandırma yapıyoruz.\n",
    "df.groupBy(\"names\").count().sort(\"count\",ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iki giriş olan Jennifer Wood'un sağlamasını yapıyoruz.\n",
    "df.filter(df[\"names\"] == \"Jennifer Wood\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14957201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İstatistiğine bakıyoruz\n",
    "df.select('age','total_purchase','account_manager','years','num_sites','churn').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc98c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aynı veriyi pandas tablosunda görüntülemek\n",
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e825e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korelasyona bakıyoruz\n",
    "df.select('age','total_purchase','account_manager','years','num_sites','churn').corr().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boş verileri görmek için\n",
    "df.filter(df.isnull() | isnan(df)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boş verileri çıkarmak için\n",
    "spark_df=spark_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri tiplerine bakacağız \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94182e",
   "metadata": {},
   "source": [
    "## 2. **Veri Ön İşleme:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c798bf",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2233f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hedef sutunumuz olan \"Churn\"u Classification algoritmasına vermeden önce int e çevirmeliyiz ki işleyebilsin.\n",
    "# \"Churn\" adlı kategorik sütunu sayısal etiketlere dönüştürüp, \"label\" adında yeni bir sütun oluşturulur.\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer=StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "\n",
    "indexed=stringIndexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Çevirdiğimizi df'ye atıyoruz\n",
    "df = indexed.withColumn(\"label\",indexed.label.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9128727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kontrol edelim\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ad67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn dışındaki diğer sutunlar için de type değiştirme yapıyoruz ama Sadece veri tipi değişikliği var.\n",
    "# Yeniden adlandırma ayrı bir işlem olarak gerçekleştirilmez.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df=df.select([col(column).cast(\"integer\").alias(column)\n",
    "              if column in [\"index\",\"Age\",'Total_Purchase','Account_Manager','Years','Num_Sites','Churn'] else col(column)\n",
    "              for column in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "# artık bütün veriler sayısal değerlere dönüştü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hedefimiz dışındaki tüm sutunları bir vektöre atıyoruz\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "depended=[\"Age\",'Total_Purchase','Account_Manager','Years','Num_Sites']\n",
    "vectorAssembler=VectorAssembler(inputCols=depended, outputCol=\"features\")\n",
    "features_vect=vectorAssembler.transform(df)\n",
    "features_vect.show()\n",
    "final=features_vect.select(\"features\",\"label\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674e687",
   "metadata": {},
   "source": [
    "Yukarıdaki hedef dışındakileri vektöre atamanın faydalarını şöyle sıralayabilirz:\n",
    "\n",
    "Veriyi Hazırlama: Birçok makine öğrenimi algoritması, bağımsız değişkenleri vektörler veya matrisler olarak bekler. Bu kodlar, belirtilen bağımsız değişkenleri bir araya getirerek, veriyi algoritmaya hazır bir formatta dönüştürmek amacıyla kullanılır.\n",
    "\n",
    "Özellik Mühendisliği: Bağımsız değişkenlerin özelliklerini vektörel bir özellikte birleştirerek, modelin daha fazla bilgiyle eğitilmesini sağlar. Özellikle bazı modeller, bağımsız değişkenlerin etkileşimlerini veya kombinasyonlarını daha iyi yakalayarak daha iyi sonuçlar üretebilir.\n",
    "\n",
    "Veri Boyutunu Azaltma: Eğer çok sayıda bağımsız değişkeniniz varsa, bunları bir vektörel özellik olarak birleştirerek veri boyutunu azaltabilirsiniz. Bu, gereksiz boyut artışını engelleyerek, daha hızlı ve daha verimli model eğitimi sağlar.\n",
    "\n",
    "Bazı Algoritmalar için Gereklilik: Özellikle bazı algoritmalar, bağımsız değişkenleri vektör formatında almayı bekler. Bu kodlar, bu tür algoritmalarla uyumlu hale getirir.\n",
    "\n",
    "Kodun Okunabilirliği: Bu kodlar, özellik mühendisliği adımını daha kolay anlaşılır ve düzenli bir şekilde gerçekleştirmenizi sağlar.\n",
    "\n",
    "Sonuç olarak, bu kodlar, belirtilen bağımsız değişkenleri birleştirip bir vektörel özellik sütunu oluşturarak veriyi modele hazır hale getirir. Bu tür bir özellik mühendisliği, genellikle modelinizin performansını artırmanıza ve daha iyi sonuçlar elde etmenize yardımcı olabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2321f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"age_sqrt\",df.Age**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3519ecd",
   "metadata": {},
   "source": [
    "Bu kod, DataFrame'deki \"Age\" (yaş) sütunundaki değerleri kullanarak yeni bir sütun olan \"age_sqrt\" (yaşın karesi) sütununu oluşturmak için kullanılır. Bu tür özellik mühendisliği işlemleri, veriyi daha iyi anlamak, modele daha fazla bilgi sağlamak veya bazı modellerin performansını artırmak amacıyla gerçekleştirilir. İşte kodun amaçları ve faydaları:\n",
    "\n",
    "1. **Model İçin Daha İyi Özellik Sağlama:** Bazı durumlarda, yaş değeriyle birlikte yaşın karesi de modelinize daha fazla bilgi sağlayabilir. Örneğin, bazı davranışlar yaşın karesiyle daha iyi açıklanabilir. Model, yaşın karesini kullanarak daha karmaşık ilişkileri yakalamak veya bazı örüntüleri keşfetmek için daha iyi bir performans gösterebilir.\n",
    "\n",
    "2. **Non-Lineer İlişkileri Temsil Etme:** Modeliniz, sadece yaşın kendisi yerine yaşın karesi ile daha iyi non-lineer ilişkileri temsil edebilir. Özellikle doğrusal olmayan ilişkileri modellemek için bu tür özellikleri kullanmak faydalı olabilir.\n",
    "\n",
    "3. **Azalan Önemli Etki:** Yaşın karesi, yaş arttıkça yaşın kendisinin etkisinin azaldığı bir durumu temsil edebilir. Örneğin, yaşın gençken daha belirgin olduğu bir etki sonrasında yaşın artmasıyla etkisinin azaldığı bir senaryo düşünülebilir. Bu tür durumları yakalamak için bu tür dönüşümler yapmak yararlı olabilir.\n",
    "\n",
    "4. **Hem Sayısal Hem de Kategorik Değerler İçin Uygunluk:** Yaş değeri kategorik olmadığından ve genellikle sürekli bir değişken olduğundan, yaşın karesi gibi türetilmiş özellikler bu tür durumlar için daha uygundur.\n",
    "\n",
    "Ancak, her durumda özellik mühendisliği yapmadan önce dikkatli bir analiz ve modelinize uygunluğunu değerlendirmek önemlidir. Özellikle gereksiz veya anlamsız özellikler modelinizi karmaşıklaştırabilir ve performansını düşürebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c1795",
   "metadata": {},
   "source": [
    "## 3. **Veri Bölünmesi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData)=final.randomSplit([0.7,0.3],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f639e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be27389",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836c209",
   "metadata": {},
   "source": [
    "## 4. **Model Oluşturma:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b959c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm=GBTClassifier(maxIter=70,maxDepth=20,featuresCol=\"features\",labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72296f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model=gbm.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49896391",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7f8d8",
   "metadata": {},
   "source": [
    "## 5. **Model Değerlendirmesi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57547",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=evaluator.evaluate(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba993054",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa02a36",
   "metadata": {},
   "source": [
    "## 6. **Model Doğrulama:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b9ac2",
   "metadata": {},
   "source": [
    "Bu kod, çapraz doğrulama için bir Gradient Boosted Trees sınıflandırma modeli oluştururken denenecek olan farklı \"maxIter\" ve \"maxDepth\" parametre kombinasyonlarını belirler. Bu kombinasyonlar çapraz doğrulama işlemi sırasında modelin farklı parametre ayarlarıyla performansını değerlendirmenizi sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b098af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_cv=GBTClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "      .addGrid(gbm.maxIter, [10,20]) \\\n",
    "      .addGrid(gbm.maxDepth,[5,10]) \\\n",
    "      .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv=CrossValidator(estimator=gbm_cv,\n",
    "                  estimatorParamMaps=paramGrid,\n",
    "                  evaluator=evaluator,\n",
    "                  numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb099ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel=cv.fit(trainData)\n",
    "\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bestModel.transform(testData)\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Test verilerinin doğruluğu: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856044aa",
   "metadata": {},
   "source": [
    "## 7. **Sonuçların Sunumu:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02755059",
   "metadata": {},
   "source": [
    "Sonuç olarak bu proje kapsamında, PySpark kullanarak büyük veri üzerinde churn tahmini gerçekleştiren bir makine öğrenimi modeli oluşturduk. Veriyi okuma, ön işleme, özellik mühendisliği, model oluşturma ve hiperparametre ayarı gibi adımları takip ettik. Oluşturduğumuz Gradient Boosted Trees (GBT) sınıflandırma modelini farklı hiperparametre kombinasyonlarıyla çapraz doğrulama yöntemiyle değerlendirdik. Sonuçlarımızı MulticlassClassificationEvaluator kullanarak doğruluk metriği üzerinden değerlendirdik. Bu proje, büyük veri setleri üzerinde PySpark'ın yeteneklerini kullanarak veri analizi ve makine öğrenimi uygulamalarını gerçekleştirebileceğimizi gösterdi. Projede edindiğimiz deneyimle, gelecekteki büyük veri projelerine daha iyi yaklaşabileceğimiz ve daha gelişmiş analizler gerçekleştirebileceğimiz bir temel oluşturduk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitilen GBT modelini seçip\n",
    "trained_model = cvModel.bestModel\n",
    "\n",
    "# Modeli \"11-Spark Big Data Machine Learning\" adlı dosyaya kaydet\n",
    "saved_model = \"Project Based Learning Level-2/11-Spark Big Data Machine Learning/my_model\"\n",
    "trained_model.save(saved_model)\n",
    "\n",
    "# Kaydedilen modeli yüklemek için:\n",
    "# loaded_model = GBTClassifierModel.load(saved_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
